{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef7d24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_INDEX_NAME = \"\"\n",
    "PINECONE_ENVIRONMENT = \"\"\n",
    "PINECONE_API_KEY = \"\"\n",
    "PINECONE_NAMESPACE = \"\"\n",
    "MANUAL_NAME = \"\"\n",
    "\n",
    "MAX_TEXT_EMBEDDING_ARRAY_SIZE = 100\n",
    "\n",
    "EMBEDDING_MODEL = \"\"\n",
    "OPEN_AI_API_KEY = \"\"\n",
    "OPENAI_ORGANIZATION = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bff35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "import openai\n",
    "\n",
    "openai.api_key = OPEN_AI_API_KEY\n",
    "openai.organization = OPENAI_ORGANIZATION\n",
    "\n",
    "CHUNK_SIZE = 200\n",
    "\n",
    "def chunk_document(text, tokenizer):\n",
    "    tokens = tokenizer.encode(text)\n",
    "\n",
    "    \"\"\"Yield successive n-sized chunks from text.\"\"\"\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        # Find the nearest end of sentence within a range of 0.5 * n and 1.5 * n tokens\n",
    "        j = min(i + int(1.5 * CHUNK_SIZE), len(tokens))\n",
    "        while j > i + int(0.5 * CHUNK_SIZE):\n",
    "            # Decode the tokens and check for full stop or newline\n",
    "            chunk = tokenizer.decode(tokens[i:j])\n",
    "            if chunk.endswith(\".\") or chunk.endswith(\"\\n\"):\n",
    "                break\n",
    "            j -= 1\n",
    "        # If no end of sentence found, use n tokens as the chunk size\n",
    "        if j == i + int(0.5 * CHUNK_SIZE):\n",
    "            j = min(i + CHUNK_SIZE, len(tokens))\n",
    "        yield tokens[i:j]\n",
    "        i = j\n",
    "\n",
    "def get_embeddings(text_array, engine):\n",
    "    max_retries = 5 # Maximum number of retries\n",
    "    base_delay = 1 # Base delay in seconds\n",
    "    factor = 2 # Factor to multiply the delay by after each retry\n",
    "    while True:\n",
    "        try:\n",
    "            return openai.Engine(id=engine).embeddings(input=text_array)[\"data\"]\n",
    "        except Exception as e:\n",
    "            if max_retries > 0:\n",
    "                logging.info(f\"Request failed. Retrying in {base_delay} seconds.\")\n",
    "                time.sleep(base_delay)\n",
    "                max_retries -= 1\n",
    "                base_delay *= factor\n",
    "            else:\n",
    "                raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80df437a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embeddings(text_chunks):\n",
    "    text_chunks_arrays = [text_chunks[i:i+MAX_TEXT_EMBEDDING_ARRAY_SIZE] for i in range(0, len(text_chunks), MAX_TEXT_EMBEDDING_ARRAY_SIZE)]\n",
    "\n",
    "    embeddings = []\n",
    "    for text_chunks_array in text_chunks_arrays:\n",
    "        embeddings_response = get_embeddings(text_chunks_array, \"text-embedding-ada-002\")\n",
    "        embeddings.extend([embedding[\"embedding\"] for embedding in embeddings_response])\n",
    "    \n",
    "    text_embeddings = list(zip(text_chunks, embeddings))\n",
    "\n",
    "    return text_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1139c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx2txt\n",
    "import tiktoken\n",
    "import logging\n",
    "import pinecone\n",
    "\n",
    "def create_embeddings_for_doc(filename, doc_x_file):\n",
    "    file_text_dict = {}\n",
    "\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    extracted_text = docx2txt.process(doc_x_file)\n",
    "    file_text_dict[filename] = extracted_text\n",
    "\n",
    "    clean_file_string = extracted_text.replace(\"\\n\", \"; \").replace(\"  \", \" \")\n",
    "\n",
    "    text_to_embed = \"Filename is: {}; {}\".format(\n",
    "        filename, clean_file_string)\n",
    "    \n",
    "    token_chunks = list(chunk_document(text_to_embed, tokenizer))\n",
    "    text_chunks = [tokenizer.decode(chunk) for chunk in token_chunks]\n",
    "    text_embeddings = build_embeddings(text_chunks)\n",
    "\n",
    "    vectors = []\n",
    "\n",
    "    for i, (text_chunk, embedding) in enumerate(text_embeddings):\n",
    "        id = str(filename+\"-!\"+str(i))\n",
    "        file_text_dict[id] = text_chunk\n",
    "        vectors.append(\n",
    "            (id, embedding, {\"filename\": filename, \"file_chunk_index\": i, \"text\": text_chunk, \"manual\": MANUAL_NAME}))\n",
    "        \n",
    "    batch_size = 100\n",
    "    batches = [vectors[i:i+batch_size] for i in range(0, len(vectors), batch_size)]\n",
    "    return batches\n",
    "\n",
    "def load_pinecone_index() -> pinecone.Index:\n",
    "    pinecone.init(\n",
    "        api_key=PINECONE_API_KEY,\n",
    "        environment=PINECONE_ENVIRONMENT,\n",
    "    )\n",
    "\n",
    "    if not PINECONE_INDEX_NAME in pinecone.list_indexes():\n",
    "        print(pinecone.list_indexes())\n",
    "        raise KeyError(f\"Index '{PINECONE_INDEX_NAME}' does not exist.\")\n",
    "    index = pinecone.Index(PINECONE_INDEX_NAME)\n",
    "\n",
    "    return index\n",
    "\n",
    "def upload_pinecone_index(filename, embeddings):\n",
    "    # Upsert to Pinecone\n",
    "    pinecone_index = load_pinecone_index()\n",
    "    for e_batch in embeddings:\n",
    "        try:\n",
    "            pinecone_index.upsert(\n",
    "                vectors=e_batch, namespace=PINECONE_NAMESPACE)\n",
    "\n",
    "            logging.info(\n",
    "                \"[handle_file_string] Upserted batch of embeddings for {}\".format(filename))\n",
    "        except Exception as e:\n",
    "            logging.error(\n",
    "                \"[handle_file_string] Error upserting batch of embeddings to Pinecone: {}\".format(e))\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f07e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment process_folder and provide folder path containing DocX files\n",
    "\n",
    "import os\n",
    "\n",
    "def process_folder(folder):\n",
    "    for filename in os.listdir(folder):\n",
    "        file = os.path.join(folder, filename)\n",
    "\n",
    "        if os.path.isfile(file):\n",
    "            print(file)\n",
    "            filename = os.path.basename(file)\n",
    "            embeddings = create_embeddings_for_doc(filename, file)\n",
    "            upload_pinecone_index(filename, embeddings)\n",
    "\n",
    "\n",
    "process_folder('./700')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f6c02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrive closest match embeddings from Pinecone and send context to Open AI completion endpoint to build answer\n",
    "import pinecone\n",
    "import openai\n",
    "\n",
    "def retrieve_context(query):\n",
    "    pinecone.init(\n",
    "        api_key=PINECONE_API_KEY,\n",
    "        environment=PINECONE_ENVIRONMENT,\n",
    "    )\n",
    "\n",
    "    index = pinecone.Index(PINECONE_INDEX_NAME)\n",
    "\n",
    "    res = openai.Embedding.create(\n",
    "        input=[query],\n",
    "        engine=EMBEDDING_MODEL\n",
    "    )\n",
    "\n",
    "    xq = res['data'][0]['embedding']\n",
    "    res = index.query(xq, top_k=5, include_metadata=True, namespace=PINECONE_NAMESPACE)\n",
    "\n",
    "    contexts = [\n",
    "        x['metadata']['text'] for x in res['matches']\n",
    "    ]\n",
    "\n",
    "    return contexts\n",
    "\n",
    "def build_prompt(query, context):\n",
    "    context_limit = 3750\n",
    "\n",
    "    prompt_start = (\n",
    "        \"Answer the question using only the context provided.\\n\\n\" +\n",
    "        \"Remember to:\\n\\n\" +\n",
    "        \"1. Present step-by-step instructions as an ordered list.\\n\\n\" +\n",
    "        \"2. Summarize the question before providing the answer.\\n\\n\" +\n",
    "        \"3. Offer comprehensive and accurate information, covering all necessary details.\\n\\n\" +\n",
    "        \"4. Supply any prerequisite context users need before starting the steps.\\n\\n\" +\n",
    "        \"5. If you're unsure of the answer, respond with: 'I'm sorry, I don't have an answer for that.\\n\\n\" +\n",
    "        \"6. Include any relevent safetyl precautions the user should take.\\n\\n\" +\n",
    "        \"7. Provide specific information, terms, or examples to enhance clarity and understanding.\\n\\n\" +\n",
    "        \"8. Include sources or reference materials (if applicable)\\n\\n\" +\n",
    "        \"9. Include a reference to where the information was found in the manual. Use the format Section - heading. \\n\\n\" +\n",
    "        \"Context:\\n\"\n",
    "    )\n",
    "    \n",
    "    prompt_end = (\n",
    "        f\"\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    )\n",
    "       \n",
    "    for i in range(1, len(context)):\n",
    "        if len(\"\\n\\n---\\n\\n\".join(context[:i])) >= context_limit:\n",
    "            prompt = (\n",
    "                prompt_start +\n",
    "                \"\\n\\n---\\n\\n\".join(context[:i-1]) +\n",
    "                prompt_end\n",
    "            )\n",
    "            break\n",
    "        elif i == len(context)-1:\n",
    "            prompt = (\n",
    "                prompt_start +\n",
    "                \"\\n\\n---\\n\\n\".join(context) +\n",
    "                prompt_end\n",
    "            )\n",
    "    return prompt\n",
    "\n",
    "def complete(prompt):\n",
    "    res = openai.Completion.create(\n",
    "        engine='text-davinci-003',\n",
    "        prompt=prompt,\n",
    "        temperature=0.2,\n",
    "        max_tokens=2000,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=None\n",
    "    )\n",
    "    return res['choices'][0]['text'].strip()\n",
    "\n",
    "def answer(query):\n",
    "    context = retrieve_context(query)\n",
    "    prompt = build_prompt(query, context)\n",
    "    answer = complete(prompt)\n",
    "    \n",
    "    print(answer)\n",
    "\n",
    "answer(\"How do I start the laser?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f3f5f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
