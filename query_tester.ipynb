{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrive closest match embeddings from Pinecone and send context to Open AI completion endpoint to build answer\n",
    "import pinecone\n",
    "import openai\n",
    "\n",
    "PINECONE_API_KEY = ''\n",
    "PINECONE_ENV = ''\n",
    "PINECONE_INDEX_NAME = ''\n",
    "PINECONE_NAMESPACE = ''\n",
    "\n",
    "OPEN_AI_API_KEY = ''\n",
    "EMBEDDING_MODEL = ''\n",
    "\n",
    "def retrieve_context(query):\n",
    "    pinecone.init(\n",
    "        api_key=PINECONE_API_KEY,\n",
    "        environment=PINECONE_ENV,\n",
    "    )\n",
    "\n",
    "    index = pinecone.Index(PINECONE_INDEX_NAME)\n",
    "\n",
    "    openai.api_key = OPEN_AI_API_KEY\n",
    "\n",
    "    res = openai.Embedding.create(\n",
    "        input=[query],\n",
    "        engine=EMBEDDING_MODEL\n",
    "    )\n",
    "\n",
    "    xq = res['data'][0]['embedding']\n",
    "    res = index.query(xq, top_k=3, include_metadata=True, namespace=PINECONE_NAMESPACE)\n",
    "\n",
    "    contexts = [\n",
    "        x['metadata']['text'] for x in res['matches']\n",
    "    ]\n",
    "\n",
    "    return contexts\n",
    "\n",
    "def build_prompt(query, context):\n",
    "    context_limit = 3750\n",
    "    prompt = \"\"\n",
    "    prompt_start = (\n",
    "        \"You are a meeting log retrieval bot. You receive questions about a library of project logs and return answers.\\n\\n\" +\n",
    "        \"Remember to:\\n\\n\" +\n",
    "        \"- Format responses with markdown.\\n\\n\" + \n",
    "        \"- Look for days in the format Month, Day, Year. For example, May 13, 2023.\\n\\n\" +\n",
    "        \"- Only return answers from th provided content below:.\\n\\n\"\n",
    "    )\n",
    "    \n",
    "    prompt_end = (\n",
    "        f\"\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    )\n",
    "       \n",
    "    for i in range(1, len(context)):\n",
    "        if len(\"\\n\\n---\\n\\n\".join(context[:i])) >= context_limit:\n",
    "            prompt = (\n",
    "                prompt_start +\n",
    "                \"\\n\\n---\\n\\n\".join(context[:i-1]) +\n",
    "                prompt_end\n",
    "            )\n",
    "            break\n",
    "        elif i == len(context)-1:\n",
    "            prompt = (\n",
    "                prompt_start +\n",
    "                \"\\n\\n---\\n\\n\".join(context) +\n",
    "                prompt_end\n",
    "            )\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def complete(prompt):\n",
    "    res = openai.Completion.create(\n",
    "        engine='text-davinci-003',\n",
    "        prompt=prompt,\n",
    "        temperature=0.2,\n",
    "        max_tokens=2000,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=None\n",
    "    )\n",
    "    return res['choices'][0]['text'].strip()\n",
    "\n",
    "def answer(query):\n",
    "    context = retrieve_context(query)\n",
    "    prompt = build_prompt(query, context)\n",
    "    answer = complete(prompt)\n",
    "    \n",
    "    print(answer)\n",
    "\n",
    "answer(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
